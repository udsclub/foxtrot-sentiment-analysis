{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "# import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from spacy.en import English\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model: \n",
    "https://drive.google.com/file/d/0B0YWufjvNmyVaFJvbllsR2lMX2M/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 4\n",
    "TRAIN_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrash = ['<hr>', '<br>', '<br />', '<p>', '<i>', '\\n', '\\t', '“', '”', '″', '…', '₤', '▼', '★★', '', '–', '`',\n",
    "          '‘', '’',  '«', '»', '®', '°', 'º', '°c', '°f', '´', '·', '½', '¾', '¿', '§', '¨', '¡', '¢', '£', '¤']\n",
    "\n",
    "redundant_signs = list(string.punctuation) + thrash\n",
    "letters = [x for x in string.ascii_lowercase + '. ']\n",
    "\n",
    "stop_words = set(stopwords.words('english') + get_stop_words('en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen,\n",
    "                        dropout=0.2))\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "nlp = English()\n",
    "def lemmatizer(line, nlp=nlp):\n",
    "    return ' '.join([word.lemma_ for word in nlp(line)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemmer\n",
    "stem = PorterStemmer()\n",
    "def stemmer(line, stem=stem):\n",
    "    return \" \".join([stem.stem(w) for w in line.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(inp_str):\n",
    "    inp_str = inp_str.lower()\n",
    "\n",
    "    # fix haven't|doesn't|shouldn't cases\n",
    "    inp_str = inp_str.replace(\"n't\", \" not\")\n",
    "    inp_str = inp_str.replace(\"'re\", \" are\")\n",
    "    inp_str = inp_str.replace(\"'s\", \" s\")\n",
    "    inp_str = inp_str.replace(\"'ve\", \" have\")\n",
    "    inp_str = inp_str.replace(\"'ll\", \" will\")\n",
    "    inp_str = inp_str.replace(\"'d\", \" d\")\n",
    "\n",
    "    # here may be actor's names, types of smth etc. I guess it's redundant info\n",
    "    # let's discuss of necessity of this block\n",
    "    bracket_words = re.findall('([\\(\\[\\{].+?[\\)\\]\\}])', inp_str)\n",
    "    for word in bracket_words:\n",
    "        inp_str = inp_str.replace(''.join(word), \"\")\n",
    "\n",
    "    # replace redundant_signs\n",
    "    for item in redundant_signs:\n",
    "        inp_str = inp_str.replace(item, ' ')\n",
    "\n",
    "    # replace digits\n",
    "    inp_str = re.sub('\\d', ' ', inp_str)\n",
    "    # replace three or more letters in a row on one. Example: aaaaaah, i like it. - > Ah, I like ite\n",
    "    inp_str = re.sub(r'(.)\\1{3,}', r'\\1', inp_str)\n",
    "    # replace one-letter words or just letters\n",
    "    inp_str = re.sub(r\"\\b[a-z]{1}\\b\", ' ', inp_str)\n",
    "\n",
    "    return ' '.join(list(filter(None, inp_str.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finalize_df(df, preprocessor=stemmer):\n",
    "    df['new_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "    df['is_ascii'] = df['new_text'].apply(lambda x: set(x).issubset(letters))\n",
    "    df = df[df['is_ascii'] == 1]\n",
    "    df = df[df['new_text'].str.len() > 2]\n",
    "\n",
    "    df['new_text'] = df['new_text'].apply(preprocessor)\n",
    "    df['new_text'] = df['new_text'].apply(lambda x: ' '.join(\n",
    "        [item for item in x.split() if item not in stop_words]))\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = df[df['new_text'].notnull()]\n",
    "    df = df.ix[:, ['label', 'new_text']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data:  (152610, 2)\n",
      "Found 85231 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load your data and process it\n",
    "data = pd.concat([pd.read_csv('reviews_rt_all.csv',sep='|'), \n",
    "                  pd.read_csv('imdb_small.csv',sep='|')], ignore_index=True)\n",
    "#data = pd.read_csv('imdb_small.csv', sep='|')  # here type your file name\n",
    "print('source data: ', data.shape)\n",
    "# print('Longest string: %d' % data['text'].str.len().max())\n",
    "\n",
    "data = finalize_df(data, lemmatizer)\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(data['new_text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116655 samples, validate on 29164 samples\n",
      "Epoch 1/4\n",
      "\r",
      "    32/116655 [..............................] - ETA: 465s - loss: 0.6932 - acc: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:92: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116655/116655 [==============================] - 25s - loss: 0.4919 - acc: 0.7583 - val_loss: 0.4367 - val_acc: 0.7995\n",
      "Epoch 2/4\n",
      "116655/116655 [==============================] - 26s - loss: 0.4369 - acc: 0.7918 - val_loss: 0.4175 - val_acc: 0.8059\n",
      "Epoch 3/4\n",
      "116655/116655 [==============================] - 25s - loss: 0.4072 - acc: 0.8092 - val_loss: 0.4133 - val_acc: 0.8072\n",
      "Epoch 4/4\n",
      "116655/116655 [==============================] - 25s - loss: 0.3802 - acc: 0.8231 - val_loss: 0.4208 - val_acc: 0.8065\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    X_train_1, X_test_1, y_train, y_test = train_test_split(data['new_text'],\n",
    "                                                            data['label'],\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=29,\n",
    "                                                            stratify=data['label'])\n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train_1)\n",
    "    sequences_test = tokenizer.texts_to_sequences(X_test_1)\n",
    "    X_train = pad_sequences(sequences_train, maxlen=maxlen)\n",
    "    X_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(X_test, y_test))\n",
    "    model.save('cnn_foxtrot.h5')\n",
    "else:\n",
    "    model = load_model('cnn_foxtrot.h5')\n",
    "    data_sequences = tokenizer.texts_to_sequences(data['new_text'])\n",
    "    X_test = pad_sequences(data_sequences, maxlen=maxlen)\n",
    "    y_test = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29088/29164 [============================>.] - ETA: 0s==============================\n",
      "****Results****\n",
      "Accuracy: 80.6542% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print('****Results****')\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {:.4%} \\n\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
